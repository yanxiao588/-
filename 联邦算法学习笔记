联邦学习是由谷歌研究院在2016年就提出的概念，指在数据不共享的情况下完成联合建模。具体来讲，各个数据拥有者的自有数据不会离开本地，通过联邦系统中加密机制下的
参数交换方式（即在不违反数据隐私法规的情况下）联合建立一个全局的共享模型，建好的模型在各自的区域只为本地的目标服务。
下面是一些联邦学习算法对比
类型            基础              算法              框架                  特点


联邦      联邦线性算法        逻辑回归               中心            同态加密，观察模型变化，周期性梯度更新
机器                          逻辑回归              去中心           取消第三方参与，有标签数据持有方主导，差分隐私
学习      
          联邦树模型         联邦森林                中心            模型分散存储，中心服务器储存结构
                             梯度上升树SecureBoost   去中心           同态加密，特征分桶聚合，保障准确率
                             梯度上升树SimFL         去中心           哈希表加密，加权梯度上升，通信效率高


联邦      联邦支持向量机       支持向量机Valentin    中心             哈希表加密，次梯度更新，隐私性较好
深度      联邦神经网络          NN                   中心            比传统神经网络收敛更快，参数联合初始化时具有更络收敛更快，参数联合初始化时具有更好的收敛效果
学习      联邦卷积神经网络      CNN                  中心             网络结构比RNN简单，收敛速度更快
                               VGG11                中心             non-IID数据上，参数压缩的优化算法收敛效果较差；不压缩的收敛效果较好，但参数量较大
          联邦LSTM             LSTM                 中心             受数据分布影响较大，不同的参数聚合方式效果不同


下面是联邦学习算法的优化分类方法
优化角度     文献方法        优化方法                                    优缺点
通信成本     FedAvg        IID 数据；                       增加参与方本地计算 增加计算成本；non-IID数据优化效果差

             FedProx      non-IID数据；增加本地计算         增加计算成本，可优化non-IID数据，代价是准确性降低

             VFL          纵向联邦算法；增加本地计算        增加计算成本，代价是降低准确性
             结构和轮廓    压缩传输模型，提升参与方到        参与方到服务器参数压缩，代价是复杂的模型结构可能出现收敛问题
             更新机制      服务器的通信效率

             服务器-客户  压缩传输模型，提升服务器           服务器到参与方参数压缩，代价是准确性降低，可能有收敛问题
             端更新       到参与方的通信效率
客户端选择   FedCS        选择迭代效率最优的模型训练参与方    比FedAvg更准确，但是只能被应用于简单的NN模型，不适合复杂模型

             Hybrid-FL    服务器选择客户端数据组成           non-IID数据收敛有问题
                          近似IID的数据集
异步聚合     AsyncFedAvg  服务器接收到客户端参数更新         存在non-IID数据收敛问题
                          就立刻聚合
             FedAsync     服务端通过加权聚合的方式           难调参数，存在收敛问
                          获取客户端的模型参数
